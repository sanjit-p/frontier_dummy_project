{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Auto ML classifier<br>\n", "*Automated machine learning is the process of automating the tasks of applying machine learning to real-world problems.<br>\n", "AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model.*<br>\n", "### Import the library"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from fluidai_net.data.data_class import Data\n", "from fluidai_net.data.data_processor import DataProcessor\n", "from fluidai_net.project_manager.modelreg_helper import ModelRegHelper\n", "from fluidai_net.project_manager.yaml_manager import YamlManager\n", "import pandas as pd\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "import numpy as np\n", "import os\n", "import joblib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Define global variables"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["input_path = r'/home/coder/PycharmProjects/FRONTIER-MAIN/TEST-SANJIT/TEST-SANJIT_V0/Data Inputs'\n", "output_path = r'/home/coder/PycharmProjects/FRONTIER-MAIN/TEST-SANJIT/TEST-SANJIT_V0/Data Outputs'\n", "model_path = r'/home/coder/PycharmProjects/FRONTIER-MAIN/TEST-SANJIT/TEST-SANJIT_V0/Model Folder'\n", "vec_meta_path = r'/home/coder/PycharmProjects/FRONTIER-MAIN/TEST-SANJIT/TEST-SANJIT_V0/Vec Meta Data'\n", "project_name = r'TEST-SANJIT'\n", "project_version = '0'\n", "model_types = ['xg_boost_classifier', 'logistic_regression']\n", "file_name = 'test.csv'\n", "training_columns = \"'sepal_length','sepal_width','petal_length','petal_width'\".replace(\"'\", \"\")\n", "training_columns = training_columns.split(',')\n", "primary_key_col = \"\".replace(\"'\", \"\")\n", "target_column = \"'species'\".replace(\"'\", \"\")\n", " # Goble Info varible\n", "model_perf ='Model Performance'\n", "cf_info ='Confusion Matrix'\n", "gini_info ='GINI Score'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Preprocessing the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" # create the data object and processor obj\n", "data_obj = Data('test_obj', debug=False)\n", "processor_obj = DataProcessor(data_obj)\n", " # read the data into memory\n", "try:\n", "    data_obj.read_data_frame_from_path(file_path=os.path.join(input_path, file_name))\n", "except Exception as err:\n", "    print(err)\n", "    data_obj.read_data_frame_from_path(file_path=os.path.join(output_path, file_name))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" # Check the target column is there in main column list and add\n", "if target_column not in training_columns:\n", "    training_columns.append(target_column)\n", " # save the data as a backup\n", "if primary_key_col != '':\n", "    primary_key_data = data_obj.data_frame[primary_key_col]\n", "    # remove the target column and primary key col\n", "    processor_obj.cleaner.drop_columns(column_values=[primary_key_col])\n", "if primary_key_col in training_columns:\n", "    training_columns.remove(primary_key_col)\n", "original = data_obj.data_frame[training_columns]\n", "processor_obj.cleaner.select_columns(column_values=training_columns)\n", "target_column_data = data_obj.data_frame[target_column]\n", "if target_column in training_columns:\n", "    training_columns.remove(target_column)\n", " # Identification of columns\n", " # check if columns are categorical or numerical\n", "categorical_columns = data_obj.data_frame[training_columns].select_dtypes('object')\n", "continuous_columns = data_obj.data_frame[training_columns].select_dtypes('number')\n", "columns_used_to_train = []\n", " # Processing of the data\n", " # Impute nan values\n", "processor_obj.stats_calculator.impute_value(columns=continuous_columns)\n", " # keep only top 10 values\n", "for categorical_col in categorical_columns:\n", "    if data_obj.data_frame[categorical_col].nunique() > 10:\n", "        processor_obj.cleaner.n_largest_categorizer(column_name=categorical_col, n_value=10)\n", " # Encoding in the data\n", " # standardize the continuous columns and one hot encode categorical cols\n", "for col in continuous_columns:\n", "    try:\n", "        processor_obj.encoder.standardize(column_name=col, default_vec_value='mean')\n", "        columns_used_to_train.append(col)\n", "    except ZeroDivisionError:\n", "        processor_obj.cleaner.drop_columns(column_values=col)\n", "for col in categorical_columns:\n", "    dummies = pd.get_dummies(data_obj.data_frame[col])\n", "    dummy_cols = [col + '|' + i for i in dummies.columns]\n", "    data_obj.data_frame[dummy_cols] = dummies\n", "    data_obj.data_frame.drop(columns=[col], inplace=True)\n", "    [columns_used_to_train.append(i) for i in dummy_cols]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" # save the encoding logic\n", "complete_data = data_obj.data_frame.drop(columns=[target_column])[columns_used_to_train]\n", "complete_target_data = data_obj.data_frame[target_column]\n", "backup_data = data_obj.data_frame.copy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["split the data into train and test sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_test_dict = processor_obj.learner.split_train_test_data(target_column, 0.2, upsample=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the data from the train test dict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train = train_test_dict['x_train']\n", "x_test = train_test_dict['x_test']\n", "y_train = train_test_dict['y_train']\n", "y_test = train_test_dict['y_test']\n", "model_train_data = x_train.copy()\n", "model_train_data[target_column] = y_train\n", "model_test_data = x_test.copy()\n", "model_test_data[target_column] = y_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the Data and performance measurement ....::"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for model_type in model_types:\n", "    try:\n", "        print(\n", "            '======================================================================================')\n", "        print('==================== ' + model_type + '====================')\n\n", "        # Train the auto ml model\n", "        trained_model = getattr(processor_obj.learner, model_type)(x_train=x_train,\n", "                                                                   y_train=y_train)\n", "        model_name = \"{0}_{1}_{2}_{3}\".format('auto_ml', model_type, file_name.split('.')[0], trained_model[\"model_name\"].split('_')[-1])\n", "        model_saver_object = ModelRegHelper(trained_model=trained_model[\"model\"],\n", "                                            model_type=\"classification\", train_data=model_train_data,\n", "                                            columns_used_to_train=columns_used_to_train, test_data=model_test_data,\n", "                                            target_column=target_column, model_name=model_name, auto=False,\n", "                                            input_path=input_path, output_path=output_path, model_path=model_path)\n", "        model_saver_object.save_classification_or_regression_model()\n", "    except Exception as err:\n", "        warnings.warn('Unable to build Model ' + model_type)\n", "        warnings.warn(str(err))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}